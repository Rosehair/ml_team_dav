{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !conda install -y nomkl > tmp.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import _pickle as pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo\n",
    "* https://github.com/Lasagne/Recipes/tree/master/modelzoo\n",
    "* More models within the community\n",
    "* Pick model, copy init, download weights\n",
    "* Here we proceed with vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/lasagne/recipes/pretrained/imagenet/vgg16.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copyright: see http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    net = {}\n",
    "    net['input'] = InputLayer((None, 3, 224, 224))\n",
    "    net['conv1_1'] = ConvLayer(\n",
    "        net['input'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['conv1_2'] = ConvLayer(\n",
    "        net['conv1_1'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['pool1'] = PoolLayer(net['conv1_2'], 2)\n",
    "    net['conv2_1'] = ConvLayer(\n",
    "        net['pool1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['conv2_2'] = ConvLayer(\n",
    "        net['conv2_1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['pool2'] = PoolLayer(net['conv2_2'], 2)\n",
    "    net['conv3_1'] = ConvLayer(\n",
    "        net['pool2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_2'] = ConvLayer(\n",
    "        net['conv3_1'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_3'] = ConvLayer(\n",
    "        net['conv3_2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['pool3'] = PoolLayer(net['conv3_3'], 2)\n",
    "    net['conv4_1'] = ConvLayer(\n",
    "        net['pool3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_2'] = ConvLayer(\n",
    "        net['conv4_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_3'] = ConvLayer(\n",
    "        net['conv4_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool4'] = PoolLayer(net['conv4_3'], 2)\n",
    "    net['conv5_1'] = ConvLayer(\n",
    "        net['pool4'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_2'] = ConvLayer(\n",
    "        net['conv5_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_3'] = ConvLayer(\n",
    "        net['conv5_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool5'] = PoolLayer(net['conv5_3'], 2)\n",
    "    net['fc6'] = DenseLayer(net['pool5'], num_units=4096)\n",
    "    net['fc6_dropout'] = DropoutLayer(net['fc6'], p=0.5)\n",
    "    net['fc7'] = DenseLayer(net['fc6_dropout'], num_units=4096)\n",
    "    net['fc7_dropout'] = DropoutLayer(net['fc7'], p=0.5)\n",
    "    net['fc8'] = DenseLayer(\n",
    "        net['fc7_dropout'], num_units=1000, nonlinearity=None)\n",
    "    net['prob'] = NonlinearityLayer(net['fc8'], softmax)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes' names are stored here\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "#for example, 10th class is ostrich:\n",
    "print(classes[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to implement two functions in the cell below.\n",
    "\n",
    "Preprocess function should take the image with shape (w, h, 3) and transform it into a tensor with shape (1, 3, 224, 224). Without this transformation, vgg19 won't be able to digest input image. \n",
    "Additionally, your preprocessing function have to rearrange channels RGB -> BGR and subtract mean values from every channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_VALUES = np.array([104, 117, 123])\n",
    "IMAGE_W = 224\n",
    "\n",
    "def preprocess(img):\n",
    "    img = img[:, :, :: -1]\n",
    "    \n",
    "    img = img - MEAN_VALUES\n",
    "    \n",
    "    #convert from [w,h,3 to 1,3,w,h]\n",
    "    img = np.transpose(img, (2, 0, 1))[None]\n",
    "    return img\n",
    "\n",
    "def deprocess(img):\n",
    "    img = img.reshape(img.shape[1:]).transpose((1, 2, 0))\n",
    "    for i in range(3):\n",
    "        img[:,:, i] += MEAN_VALUES[i]\n",
    "    return img[:, :, :: -1].astype(np.uint8)\n",
    "\n",
    "img = (np.random.rand(IMAGE_W, IMAGE_W, 3) * 256).astype(np.uint8)\n",
    "\n",
    "print(np.linalg.norm(deprocess(preprocess(img)) - img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, the number above will be small, because deprocess function is the inverse of preprocess function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vgg16.pkl', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    weights = u.load()\n",
    "    \n",
    "lasagne.layers.set_all_param_values(net['prob'], weights['param values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = T.tensor4('input')\n",
    "output = lasagne.layers.get_output(net['prob'], input_image)\n",
    "prob = theano.function([input_image], output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "Давайте проверим, что загруженная сеть работает. Для этого мы скормим ей картину альбатроса и проверим, что она правильно его распознаёт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread('sample_images/albatross.jpg')\n",
    "img = img[:, :224, :]\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = deprocess(preprocess(img))\n",
    "plt.imshow(w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prob(preprocess(img))\n",
    "\n",
    "labels = p.ravel().argsort()[-1:-6:-1]\n",
    "print('top-5 classes are:')\n",
    "for l in labels:\n",
    "    print('%3f\\t%s' % (p.ravel()[l], classes[l].split(',')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grand-quest: Dogs Vs Cats\n",
    "* original competition\n",
    "* https://www.kaggle.com/c/dogs-vs-cats\n",
    "* 25k JPEG images of various size, 2 classes (guess what)\n",
    "\n",
    "### Your main objective\n",
    "* In this seminar your goal is to fine-tune a pre-trained model to distinguish between the two rivaling animals\n",
    "* The first step is to just reuse some network layer as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/s/d61lupw909hc785/dogs_vs_cats.train.zip?dl=1 -O data.zip\n",
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for starters\n",
    "* Train sklearn model, evaluate validation accuracy (should be >80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output_features = lasagne.layers.get_output(net['fc6'], input_image)\n",
    "# extract_features = theano.function([input_image], output_features) \n",
    "\n",
    "# #extract features from images\n",
    "# from tqdm import tqdm\n",
    "# from scipy.misc import imresize\n",
    "# import os\n",
    "# X = []\n",
    "# Y = []\n",
    "\n",
    "# #this may be a tedious process. If so, store the results in some pickle and re-use them.\n",
    "# for fname in tqdm(os.listdir('train/')):\n",
    "#     y = fname.startswith(\"cat\")\n",
    "#     img = imread(\"train/\"+fname)\n",
    "#     img = preprocess(imresize(img,(IMAGE_W,IMAGE_W)))\n",
    "#     features = extract_features(img)\n",
    "#     Y.append(y)\n",
    "#     X.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_result(res, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(obj=res, file=f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = np.concatenate(X) #stack all [1xfeature] matrices into one. \n",
    "# assert X.ndim==2\n",
    "# #WARNING! the concatenate works for [1xN] matrices. If you have other format, stack them yourself.\n",
    "\n",
    "# #crop if we ended prematurely\n",
    "# Y = Y[:len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_result(X, 'X.pkl')\n",
    "# save_result(Y, 'Y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 4096), (25000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = load_data('X.pkl')\n",
    "Y = load_data('y.pkl')\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "Y = Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 4096), (15000,), (5000, 4096), (5000,), (5000, 4096), (5000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[:15000]\n",
    "Y_train = Y[:15000]\n",
    "X_val = X[15000: 20000]\n",
    "Y_val = Y[15000: 20000]\n",
    "X_test = X[20000:]\n",
    "Y_test = Y[20000:]\n",
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__load our dakka__\n",
    "![img](https://s-media-cache-ak0.pinimg.com/564x/80/a1/81/80a1817a928744a934a7d32e7c03b242.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "clf_rf = RandomForestClassifier(n_estimators=100)\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "pred_rf = clf_rf.predict(X_val)\n",
    "acc_rf = accuracy_score(Y_val, pred_rf)\n",
    "acc_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98040000000000005"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ExtraTree\n",
    "clf_et = ExtraTreesClassifier(n_estimators=100)\n",
    "clf_et.fit(X_train, Y_train)\n",
    "pred_et = clf_et.predict(X_val)\n",
    "acc_et = accuracy_score(Y_val, pred_et)\n",
    "acc_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97919999999999996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "clf_gb = GradientBoostingClassifier()\n",
    "clf_gb.fit(X_train, Y_train)\n",
    "pred_gb = clf_gb.predict(X_val)\n",
    "acc_gb = accuracy_score(Y_val, pred_gb)\n",
    "acc_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97940000000000005"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AdaBoost\n",
    "clf_ab = AdaBoostClassifier(n_estimators=100)\n",
    "clf_ab.fit(X_train, Y_train)\n",
    "pred_ab = clf_ab.predict(X_val)\n",
    "acc_ab = accuracy_score(Y_val, pred_ab)\n",
    "acc_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98560000000000003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, Y_train)\n",
    "pred_lr = clf_lr.predict(X_val)\n",
    "acc_lr = accuracy_score(Y_val, pred_lr)\n",
    "acc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98119999999999996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ridge\n",
    "clf_rc = RidgeClassifier()\n",
    "clf_rc.fit(X_train, Y_train)\n",
    "pred_rc = clf_rc.predict(X_val)\n",
    "acc_rc = accuracy_score(Y_val, pred_rc)\n",
    "acc_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "clf_svm = SVC()\n",
    "clf_svm.fit(X_train, Y_train)\n",
    "pred_svm = clf_svm.predict(X_val)\n",
    "acc_svm = accuracy_score(Y_val, pred_svm)\n",
    "acc_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different techniques, I'll use logistic regression, because it has the highest validation accuracy. Now let's see what accuracy will be on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98280000000000001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lr = clf_lr.predict(X_test)\n",
    "accuracy_score(Y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = T.matrix('input features')\n",
    "target_Y = T.vector('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "layer = lasagne.layers.InputLayer([None, 4096], input_var=input_X)\n",
    "layer = lasagne.layers.DenseLayer(layer, 128, nonlinearity=lasagne.nonlinearities.elu)\n",
    "layer = lasagne.layers.DropoutLayer(layer)\n",
    "layer = lasagne.layers.DenseLayer(layer, 128, nonlinearity=lasagne.nonlinearities.identity)\n",
    "layer = lasagne.layers.DropoutLayer(layer)\n",
    "layer = lasagne.layers.DenseLayer(layer, 64, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "layer = lasagne.layers.DenseLayer(layer, 1, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "all_weights = lasagne.layers.get_all_params(layer)\n",
    "print(all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize):\n",
    "    assert len(inputs) == len(targets)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = lasagne.layers.get_output(layer, deterministic = False)\n",
    "y_testing = lasagne.layers.get_output(layer, deterministic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.squared_error(y_predicted, target_Y).mean()\n",
    "\n",
    "accuracy = lasagne.objectives.binary_accuracy(y_testing, target_Y).mean()\n",
    "updates = lasagne.updates.adadelta(loss, all_weights, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fun = theano.function([input_X, target_Y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_Y], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50 took 3.857s\n",
      "  training loss (in-iteration):\t\t0.055508\n",
      "  train accuracy:\t\t93.87 %\n",
      "  validation accuracy:\t\t98.48 %\n",
      "Epoch 2 of 50 took 3.520s\n",
      "  training loss (in-iteration):\t\t0.016462\n",
      "  train accuracy:\t\t98.51 %\n",
      "  validation accuracy:\t\t98.58 %\n",
      "Epoch 3 of 50 took 3.338s\n",
      "  training loss (in-iteration):\t\t0.013160\n",
      "  train accuracy:\t\t98.63 %\n",
      "  validation accuracy:\t\t98.68 %\n",
      "Epoch 4 of 50 took 3.507s\n",
      "  training loss (in-iteration):\t\t0.012828\n",
      "  train accuracy:\t\t98.90 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 5 of 50 took 3.675s\n",
      "  training loss (in-iteration):\t\t0.011579\n",
      "  train accuracy:\t\t98.97 %\n",
      "  validation accuracy:\t\t98.60 %\n",
      "Epoch 6 of 50 took 3.842s\n",
      "  training loss (in-iteration):\t\t0.009981\n",
      "  train accuracy:\t\t99.10 %\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 7 of 50 took 3.253s\n",
      "  training loss (in-iteration):\t\t0.009455\n",
      "  train accuracy:\t\t99.25 %\n",
      "  validation accuracy:\t\t98.70 %\n",
      "Epoch 8 of 50 took 3.473s\n",
      "  training loss (in-iteration):\t\t0.008246\n",
      "  train accuracy:\t\t99.29 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 9 of 50 took 3.568s\n",
      "  training loss (in-iteration):\t\t0.007278\n",
      "  train accuracy:\t\t99.35 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 10 of 50 took 3.755s\n",
      "  training loss (in-iteration):\t\t0.006975\n",
      "  train accuracy:\t\t99.45 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 11 of 50 took 3.254s\n",
      "  training loss (in-iteration):\t\t0.006949\n",
      "  train accuracy:\t\t99.49 %\n",
      "  validation accuracy:\t\t98.68 %\n",
      "Epoch 12 of 50 took 3.424s\n",
      "  training loss (in-iteration):\t\t0.006259\n",
      "  train accuracy:\t\t99.51 %\n",
      "  validation accuracy:\t\t98.70 %\n",
      "Epoch 13 of 50 took 3.493s\n",
      "  training loss (in-iteration):\t\t0.005193\n",
      "  train accuracy:\t\t99.56 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 14 of 50 took 3.827s\n",
      "  training loss (in-iteration):\t\t0.005363\n",
      "  train accuracy:\t\t99.59 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 15 of 50 took 3.386s\n",
      "  training loss (in-iteration):\t\t0.005087\n",
      "  train accuracy:\t\t99.67 %\n",
      "  validation accuracy:\t\t98.52 %\n",
      "Epoch 16 of 50 took 3.263s\n",
      "  training loss (in-iteration):\t\t0.004796\n",
      "  train accuracy:\t\t99.67 %\n",
      "  validation accuracy:\t\t98.66 %\n",
      "Epoch 17 of 50 took 3.473s\n",
      "  training loss (in-iteration):\t\t0.004461\n",
      "  train accuracy:\t\t99.68 %\n",
      "  validation accuracy:\t\t98.92 %\n",
      "Epoch 18 of 50 took 4.044s\n",
      "  training loss (in-iteration):\t\t0.004336\n",
      "  train accuracy:\t\t99.72 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 19 of 50 took 3.586s\n",
      "  training loss (in-iteration):\t\t0.003889\n",
      "  train accuracy:\t\t99.70 %\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 20 of 50 took 3.466s\n",
      "  training loss (in-iteration):\t\t0.003665\n",
      "  train accuracy:\t\t99.71 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 21 of 50 took 3.580s\n",
      "  training loss (in-iteration):\t\t0.003630\n",
      "  train accuracy:\t\t99.70 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 22 of 50 took 3.998s\n",
      "  training loss (in-iteration):\t\t0.003421\n",
      "  train accuracy:\t\t99.74 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 23 of 50 took 3.774s\n",
      "  training loss (in-iteration):\t\t0.003263\n",
      "  train accuracy:\t\t99.74 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 24 of 50 took 3.385s\n",
      "  training loss (in-iteration):\t\t0.003315\n",
      "  train accuracy:\t\t99.75 %\n",
      "  validation accuracy:\t\t98.80 %\n",
      "Epoch 25 of 50 took 3.790s\n",
      "  training loss (in-iteration):\t\t0.003018\n",
      "  train accuracy:\t\t99.75 %\n",
      "  validation accuracy:\t\t98.68 %\n",
      "Epoch 26 of 50 took 3.833s\n",
      "  training loss (in-iteration):\t\t0.003015\n",
      "  train accuracy:\t\t99.76 %\n",
      "  validation accuracy:\t\t98.70 %\n",
      "Epoch 27 of 50 took 3.594s\n",
      "  training loss (in-iteration):\t\t0.002714\n",
      "  train accuracy:\t\t99.77 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 28 of 50 took 3.225s\n",
      "  training loss (in-iteration):\t\t0.002925\n",
      "  train accuracy:\t\t99.79 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 29 of 50 took 3.487s\n",
      "  training loss (in-iteration):\t\t0.002603\n",
      "  train accuracy:\t\t99.81 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 30 of 50 took 3.627s\n",
      "  training loss (in-iteration):\t\t0.002620\n",
      "  train accuracy:\t\t99.79 %\n",
      "  validation accuracy:\t\t98.68 %\n",
      "Epoch 31 of 50 took 3.787s\n",
      "  training loss (in-iteration):\t\t0.002622\n",
      "  train accuracy:\t\t99.80 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 32 of 50 took 3.269s\n",
      "  training loss (in-iteration):\t\t0.002662\n",
      "  train accuracy:\t\t99.82 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 33 of 50 took 3.492s\n",
      "  training loss (in-iteration):\t\t0.002091\n",
      "  train accuracy:\t\t99.81 %\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 34 of 50 took 3.478s\n",
      "  training loss (in-iteration):\t\t0.002217\n",
      "  train accuracy:\t\t99.81 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 35 of 50 took 3.860s\n",
      "  training loss (in-iteration):\t\t0.002274\n",
      "  train accuracy:\t\t99.82 %\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 36 of 50 took 3.416s\n",
      "  training loss (in-iteration):\t\t0.002230\n",
      "  train accuracy:\t\t99.83 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 37 of 50 took 3.430s\n",
      "  training loss (in-iteration):\t\t0.002076\n",
      "  train accuracy:\t\t99.83 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 38 of 50 took 3.420s\n",
      "  training loss (in-iteration):\t\t0.001995\n",
      "  train accuracy:\t\t99.85 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 39 of 50 took 3.771s\n",
      "  training loss (in-iteration):\t\t0.001772\n",
      "  train accuracy:\t\t99.85 %\n",
      "  validation accuracy:\t\t98.70 %\n",
      "Epoch 40 of 50 took 3.775s\n",
      "  training loss (in-iteration):\t\t0.001811\n",
      "  train accuracy:\t\t99.86 %\n",
      "  validation accuracy:\t\t98.68 %\n",
      "Epoch 41 of 50 took 3.368s\n",
      "  training loss (in-iteration):\t\t0.001648\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.74 %\n",
      "Epoch 42 of 50 took 3.496s\n",
      "  training loss (in-iteration):\t\t0.001663\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 43 of 50 took 4.161s\n",
      "  training loss (in-iteration):\t\t0.001646\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.82 %\n",
      "Epoch 44 of 50 took 3.633s\n",
      "  training loss (in-iteration):\t\t0.001638\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.76 %\n",
      "Epoch 45 of 50 took 3.337s\n",
      "  training loss (in-iteration):\t\t0.001611\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 46 of 50 took 3.499s\n",
      "  training loss (in-iteration):\t\t0.001599\n",
      "  train accuracy:\t\t99.88 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 47 of 50 took 3.774s\n",
      "  training loss (in-iteration):\t\t0.001744\n",
      "  train accuracy:\t\t99.88 %\n",
      "  validation accuracy:\t\t98.58 %\n",
      "Epoch 48 of 50 took 3.605s\n",
      "  training loss (in-iteration):\t\t0.001705\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t98.70 %\n",
      "Epoch 49 of 50 took 3.261s\n",
      "  training loss (in-iteration):\t\t0.001427\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t98.72 %\n",
      "Epoch 50 of 50 took 3.462s\n",
      "  training loss (in-iteration):\t\t0.001427\n",
      "  train accuracy:\t\t99.89 %\n",
      "  validation accuracy:\t\t98.72 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 50 #amount of passes through the data\n",
    "batch_size = 1000 #number of samples processed at each function call\n",
    "\n",
    "accuracy_vals = []\n",
    "test_vals = []\n",
    "validation_vals = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, Y_train, batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, Y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    \n",
    "    accuracy_vals.append(train_err / train_batches)\n",
    "    test_vals.append(train_acc / train_batches * 100)\n",
    "    validation_vals.append(val_acc / val_batches * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.9864)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fun(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've got 98.64% accuracy. Let's try Cats vs dogs competition's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = load_data('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = theano.function([input_X], y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(test_data)\n",
    "pred = pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def save(pred, fname):\n",
    "    with open(fname,'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(('id','label'))\n",
    "        for i in range(len(pred)):\n",
    "            writer.writerow((i + 1,1 - pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(pred, 'predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After submitting to kaggle we've got 0.09768 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to train out network with all of our data and then try again. Maybe with more training examples our network will guess more accuratly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 50 #amount of passes through the data\n",
    "batch_size = 1000 #number of samples processed at each function call\n",
    "\n",
    "def train(X, Y):\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X, Y, batch_size):\n",
    "            inputs, targets = batch\n",
    "            train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "            train_err += train_err_batch\n",
    "            train_acc += train_acc_batch\n",
    "            train_batches += 1\n",
    "\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "        print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "            train_acc / train_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(test_data)\n",
    "pred = pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save(pred, 'predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We've got 0.10422 score. So may be we've overfitted the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "layer = lasagne.layers.InputLayer([None, 4096], input_var=input_X)\n",
    "layer = lasagne.layers.DropoutLayer(layer, p=0.7)\n",
    "layer = lasagne.layers.DenseLayer(layer, 128, nonlinearity=lasagne.nonlinearities.elu)\n",
    "layer = lasagne.layers.DropoutLayer(layer, p=0.7)\n",
    "layer = lasagne.layers.DenseLayer(layer, 128, nonlinearity=lasagne.nonlinearities.elu)\n",
    "# layer = lasagne.layers.DropoutLayer(layer)\n",
    "layer = lasagne.layers.DenseLayer(layer, 64, nonlinearity=lasagne.nonlinearities.elu)\n",
    "# layer = lasagne.layers.DropoutLayer(layer)\n",
    "layer = lasagne.layers.DenseLayer(layer, 1, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "all_weights = lasagne.layers.get_all_params(layer)\n",
    "print(all_weights)\n",
    "\n",
    "y_predicted = lasagne.layers.get_output(layer, deterministic = False)\n",
    "y_testing = lasagne.layers.get_output(layer, deterministic = True)\n",
    "\n",
    "loss = lasagne.objectives.squared_error(y_predicted, target_Y).mean()\n",
    "\n",
    "accuracy = lasagne.objectives.binary_accuracy(y_testing, target_Y).mean()\n",
    "updates = lasagne.updates.adadelta(loss, all_weights, learning_rate=0.5)\n",
    "\n",
    "train_fun = theano.function([input_X, target_Y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_Y], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50 took 14.895s\n",
      "  training loss (in-iteration):\t\t0.110726\n",
      "  train accuracy:\t\t93.30 %\n",
      "Epoch 2 of 50 took 15.093s\n",
      "  training loss (in-iteration):\t\t0.033144\n",
      "  train accuracy:\t\t97.74 %\n",
      "Epoch 3 of 50 took 15.062s\n",
      "  training loss (in-iteration):\t\t0.025214\n",
      "  train accuracy:\t\t98.12 %\n",
      "Epoch 4 of 50 took 15.066s\n",
      "  training loss (in-iteration):\t\t0.021070\n",
      "  train accuracy:\t\t98.32 %\n",
      "Epoch 5 of 50 took 15.340s\n",
      "  training loss (in-iteration):\t\t0.019751\n",
      "  train accuracy:\t\t98.41 %\n",
      "Epoch 6 of 50 took 14.555s\n",
      "  training loss (in-iteration):\t\t0.019007\n",
      "  train accuracy:\t\t98.43 %\n",
      "Epoch 7 of 50 took 14.578s\n",
      "  training loss (in-iteration):\t\t0.017879\n",
      "  train accuracy:\t\t98.48 %\n",
      "Epoch 8 of 50 took 14.627s\n",
      "  training loss (in-iteration):\t\t0.017614\n",
      "  train accuracy:\t\t98.55 %\n",
      "Epoch 9 of 50 took 14.870s\n",
      "  training loss (in-iteration):\t\t0.016691\n",
      "  train accuracy:\t\t98.63 %\n",
      "Epoch 10 of 50 took 14.577s\n",
      "  training loss (in-iteration):\t\t0.016184\n",
      "  train accuracy:\t\t98.60 %\n",
      "Epoch 11 of 50 took 14.765s\n",
      "  training loss (in-iteration):\t\t0.015954\n",
      "  train accuracy:\t\t98.68 %\n",
      "Epoch 12 of 50 took 14.716s\n",
      "  training loss (in-iteration):\t\t0.016085\n",
      "  train accuracy:\t\t98.64 %\n",
      "Epoch 13 of 50 took 14.590s\n",
      "  training loss (in-iteration):\t\t0.015205\n",
      "  train accuracy:\t\t98.71 %\n",
      "Epoch 14 of 50 took 14.795s\n",
      "  training loss (in-iteration):\t\t0.014931\n",
      "  train accuracy:\t\t98.72 %\n",
      "Epoch 15 of 50 took 14.573s\n",
      "  training loss (in-iteration):\t\t0.014245\n",
      "  train accuracy:\t\t98.77 %\n",
      "Epoch 16 of 50 took 14.951s\n",
      "  training loss (in-iteration):\t\t0.014718\n",
      "  train accuracy:\t\t98.77 %\n",
      "Epoch 17 of 50 took 14.583s\n",
      "  training loss (in-iteration):\t\t0.014123\n",
      "  train accuracy:\t\t98.80 %\n",
      "Epoch 18 of 50 took 14.752s\n",
      "  training loss (in-iteration):\t\t0.014284\n",
      "  train accuracy:\t\t98.80 %\n",
      "Epoch 19 of 50 took 14.378s\n",
      "  training loss (in-iteration):\t\t0.014328\n",
      "  train accuracy:\t\t98.81 %\n",
      "Epoch 20 of 50 took 14.449s\n",
      "  training loss (in-iteration):\t\t0.014751\n",
      "  train accuracy:\t\t98.81 %\n",
      "Epoch 21 of 50 took 14.739s\n",
      "  training loss (in-iteration):\t\t0.014081\n",
      "  train accuracy:\t\t98.82 %\n",
      "Epoch 22 of 50 took 14.720s\n",
      "  training loss (in-iteration):\t\t0.014538\n",
      "  train accuracy:\t\t98.84 %\n",
      "Epoch 23 of 50 took 14.567s\n",
      "  training loss (in-iteration):\t\t0.012993\n",
      "  train accuracy:\t\t98.85 %\n",
      "Epoch 24 of 50 took 14.673s\n",
      "  training loss (in-iteration):\t\t0.013249\n",
      "  train accuracy:\t\t98.87 %\n",
      "Epoch 25 of 50 took 15.134s\n",
      "  training loss (in-iteration):\t\t0.013132\n",
      "  train accuracy:\t\t98.86 %\n",
      "Epoch 26 of 50 took 15.111s\n",
      "  training loss (in-iteration):\t\t0.013196\n",
      "  train accuracy:\t\t98.88 %\n",
      "Epoch 27 of 50 took 14.624s\n",
      "  training loss (in-iteration):\t\t0.013766\n",
      "  train accuracy:\t\t98.91 %\n",
      "Epoch 28 of 50 took 14.618s\n",
      "  training loss (in-iteration):\t\t0.013126\n",
      "  train accuracy:\t\t98.91 %\n",
      "Epoch 29 of 50 took 14.866s\n",
      "  training loss (in-iteration):\t\t0.013512\n",
      "  train accuracy:\t\t98.92 %\n",
      "Epoch 30 of 50 took 15.012s\n",
      "  training loss (in-iteration):\t\t0.013101\n",
      "  train accuracy:\t\t98.94 %\n",
      "Epoch 31 of 50 took 14.671s\n",
      "  training loss (in-iteration):\t\t0.012663\n",
      "  train accuracy:\t\t98.96 %\n",
      "Epoch 32 of 50 took 14.516s\n",
      "  training loss (in-iteration):\t\t0.013274\n",
      "  train accuracy:\t\t98.96 %\n",
      "Epoch 33 of 50 took 14.811s\n",
      "  training loss (in-iteration):\t\t0.012765\n",
      "  train accuracy:\t\t98.95 %\n",
      "Epoch 34 of 50 took 14.721s\n",
      "  training loss (in-iteration):\t\t0.013249\n",
      "  train accuracy:\t\t98.98 %\n",
      "Epoch 35 of 50 took 14.866s\n",
      "  training loss (in-iteration):\t\t0.012328\n",
      "  train accuracy:\t\t98.96 %\n",
      "Epoch 36 of 50 took 14.738s\n",
      "  training loss (in-iteration):\t\t0.013177\n",
      "  train accuracy:\t\t98.98 %\n",
      "Epoch 37 of 50 took 14.804s\n",
      "  training loss (in-iteration):\t\t0.012681\n",
      "  train accuracy:\t\t99.00 %\n",
      "Epoch 38 of 50 took 15.093s\n",
      "  training loss (in-iteration):\t\t0.013148\n",
      "  train accuracy:\t\t99.00 %\n",
      "Epoch 39 of 50 took 14.577s\n",
      "  training loss (in-iteration):\t\t0.012790\n",
      "  train accuracy:\t\t99.02 %\n",
      "Epoch 40 of 50 took 14.819s\n",
      "  training loss (in-iteration):\t\t0.012832\n",
      "  train accuracy:\t\t98.98 %\n",
      "Epoch 41 of 50 took 14.803s\n",
      "  training loss (in-iteration):\t\t0.012576\n",
      "  train accuracy:\t\t99.02 %\n",
      "Epoch 42 of 50 took 14.639s\n",
      "  training loss (in-iteration):\t\t0.012552\n",
      "  train accuracy:\t\t99.02 %\n",
      "Epoch 43 of 50 took 14.752s\n",
      "  training loss (in-iteration):\t\t0.012817\n",
      "  train accuracy:\t\t99.01 %\n",
      "Epoch 44 of 50 took 14.741s\n",
      "  training loss (in-iteration):\t\t0.012182\n",
      "  train accuracy:\t\t99.03 %\n",
      "Epoch 45 of 50 took 14.965s\n",
      "  training loss (in-iteration):\t\t0.012812\n",
      "  train accuracy:\t\t99.05 %\n",
      "Epoch 46 of 50 took 14.805s\n",
      "  training loss (in-iteration):\t\t0.012037\n",
      "  train accuracy:\t\t99.02 %\n",
      "Epoch 47 of 50 took 14.699s\n",
      "  training loss (in-iteration):\t\t0.012074\n",
      "  train accuracy:\t\t99.06 %\n",
      "Epoch 48 of 50 took 14.551s\n",
      "  training loss (in-iteration):\t\t0.012613\n",
      "  train accuracy:\t\t99.06 %\n",
      "Epoch 49 of 50 took 14.724s\n",
      "  training loss (in-iteration):\t\t0.012254\n",
      "  train accuracy:\t\t99.08 %\n",
      "Epoch 50 of 50 took 15.131s\n",
      "  training loss (in-iteration):\t\t0.012788\n",
      "  train accuracy:\t\t99.08 %\n"
     ]
    }
   ],
   "source": [
    "train(X / 100, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(test_data)\n",
    "pred = pred.flatten()\n",
    "save(pred, 'full_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Main quest\n",
    "\n",
    "* Get the score improved!\n",
    "\n",
    "No methods are illegal: ensembling, data augmentation, NN hacks. \n",
    "Just don't let test data slip into training.\n",
    "\n",
    "The main requirement is that you implement the NN fine-tuning recipe:\n",
    "### Split the raw image data\n",
    "  * please do train/validation/test instead of just train/test\n",
    "  * reasonable but not optimal split is 20k/2.5k/2.5k or 15k/5k/5k\n",
    "### Choose which vgg layers are you going to use\n",
    "  * Anything but for prob is okay\n",
    "  * Do not forget that vgg16 uses dropout\n",
    "### Build a few layers on top of chosen \"neck\" layers.\n",
    "  * a good idea is to just stack more layers inside the same network\n",
    "  * alternative: stack on top of get_output\n",
    "### Train the newly added layers for some iterations\n",
    "  * you can selectively train some weights by only sending them to your optimizer\n",
    "      * `lasagne.updates.mysupermegaoptimizer(loss, only_those_weights_i_wanna_train)`\n",
    "  * selecting all weights from the head but not below the neck:\n",
    "      * `all_params = lasagne.layers.get_all_params(new_output_layer_or_layers,trainable=True)`\n",
    "      * `old_params= lasagne.layers.get_all_params(neck_layers,trainable=True)`\n",
    "      * `new_params = [w for w in all_params if w not in old_params]`\n",
    "  * it's cruicial to monitor the network performance at this and following steps\n",
    "### Fine-tune the network body\n",
    "  * probably a good idea to SAVE your new network weights now 'cuz it's easy to mess things up.\n",
    "  * Moreover, saving weights periodically is a no-nonsense idea\n",
    "  * even more cruicial to monitor validation performance\n",
    "  * main network body may need a separate, much lower learning rate\n",
    "      * since updates are dictionaries, one can just compute union\n",
    "      * `updates = {}`\n",
    "      * `updates.update(lasagne.updates.how_i_optimize_old_weights())`\n",
    "      * `updates.update(lasagne.updates.how_i_optimize_old_weights())`\n",
    "      * make sure they do not have overlapping keys. Otherwise, earlier one will be forgotten.\n",
    "      * `assert len(updates) == len(old_updates) + len(new_updates)`\n",
    "### PROFIT!!!\n",
    "  * Evaluate the final score\n",
    "  * Submit to kaggle\n",
    "      * competition page https://www.kaggle.com/c/dogs-vs-cats\n",
    "      * get test data https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "  \n",
    "## Some ways to get bonus points\n",
    "* explore other networks from the model zoo\n",
    "* play with architecture\n",
    "* 85%/90%/93%/95%/97% kaggle score (screen pls).\n",
    "* data augmentation, prediction-time data augmentation\n",
    "* use any more advanced fine-tuning technique you know/read anywhere\n",
    "* ml hacks that benefit the final score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
